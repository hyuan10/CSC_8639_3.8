difference_20_21 = c(" -26%"," -128%"," 46%"," -3%"," -33%"," -26%"," -93%"," 50%"," -68%"," 74%"," -37%"," 24%"," -196%"),
difference_21_22 = c(" 77%"," -32%"," 17%"," 33%"," 16%"," 48%"," 14%"," -52%"," 42%"," -235%"," -34%"," -20%"," 21%"))
mean_salary <- data.frame(
job_title = c("AI Scientist","Business Data Analyst","Data Analyst"), mean_2020 = c("45,896.00","135,000.00","45,547.29","88,162.00","190,200.00","89,185.60","325,000.00","90,500.00","125,389.80","50,180.00","260,000.00","148,261.00","246,000.00"),mean_2021 = c("160,000.00","44,677.00","100,550.74","127,187.27","170,196.60","137,136.68","196,979.00","118,187.00","129,451.94","58,255.00","141,766.67","162,674.00","105,569.00"),mean_2022 = c("160,000.00","44,677.00","100,550.74","127,187.27","170,196.60","137,136.68","196,979.00","118,187.00","129,451.94","58,255.00","141,766.67","162,674.00","105,569.00"),difference_20_21 = c(" -26%"," -128%"," 46%"," -3%"," -33%"," -26%"," -93%"," 50%"," -68%"," 74%"," -37%"," 24%"," -196%"),difference_21_22 = c(" 77%"," -32%"," 17%"," 33%"," 16%"," 48%"," 14%"," -52%"," 42%"," -235%"," -34%"," -20%"," 21%"))
mean_salary <- data.frame(job_title = c("AI Scientist","Business Data Analyst","Data Analyst"), mean_2020 = c("45,896.00","135,000.00","45,547.29","88,162.00","190,200.00","89,185.60","325,000.00","90,500.00","125,389.80","50,180.00","260,000.00","148,261.00","246,000.00"),mean_2021 = c("160,000.00","44,677.00","100,550.74","127,187.27","170,196.60","137,136.68","196,979.00","118,187.00","129,451.94","58,255.00","141,766.67","162,674.00","105,569.00"),mean_2022 = c("160,000.00","44,677.00","100,550.74","127,187.27","170,196.60","137,136.68","196,979.00","118,187.00","129,451.94","58,255.00","141,766.67","162,674.00","105,569.00"),difference_20_21 = c(" -26%"," -128%"," 46%"," -3%"," -33%"," -26%"," -93%"," 50%"," -68%"," 74%"," -37%"," 24%"," -196%"),difference_21_22 = c(" 77%"," -32%"," 17%"," 33%"," 16%"," 48%"," 14%"," -52%"," 42%"," -235%"," -34%"," -20%"," 21%"))
mean_salary <- data.frame(job_title = c("AI Scientist","Business Data Analyst","Data Analyst","Data Engineer","Data Science Manager","Data Scientist","Director of Data Science","Lead Data Engineer","Machine Learning Engineer","Machine Learning Infrastructure Engineer","Machine Learning Scientist","Principal Data Scientist","Research Scientist"), mean_2020 = c("45,896.00","135,000.00","45,547.29","88,162.00","190,200.00","89,185.60","325,000.00","90,500.00","125,389.80","50,180.00","260,000.00","148,261.00","246,000.00"),mean_2021 = c("160,000.00","44,677.00","100,550.74","127,187.27","170,196.60","137,136.68","196,979.00","118,187.00","129,451.94","58,255.00","141,766.67","162,674.00","105,569.00"),mean_2022 = c("160,000.00","44,677.00","100,550.74","127,187.27","170,196.60","137,136.68","196,979.00","118,187.00","129,451.94","58,255.00","141,766.67","162,674.00","105,569.00"),difference_20_21 = c(" -26%"," -128%"," 46%"," -3%"," -33%"," -26%"," -93%"," 50%"," -68%"," 74%"," -37%"," 24%"," -196%"),difference_21_22 = c(" 77%"," -32%"," 17%"," 33%"," 16%"," 48%"," 14%"," -52%"," 42%"," -235%"," -34%"," -20%"," 21%"))
View(mean_salary)
mean_salary <- data.frame(job_title = c("AI Scientist","Business Data Analyst","Data Analyst","Data Engineer","Data Science Manager","Data Scientist","Director of Data Science","Lead Data Engineer","Machine Learning Engineer","Machine Learning Infrastructure Engineer","Machine Learning Scientist","Principal Data Scientist","Research Scientist"), mean_2020 = c("45,896.00","135,000.00","45,547.29","88,162.00","190,200.00","89,185.60","325,000.00","90,500.00","125,389.80","50,180.00","260,000.00","148,261.00","246,000.00"),mean_2021 = c("36,526.50","59,102.00","83,827.38","85,758.00","143,126.50","70,671.73","168,707.80","179,720.00","74,611.22","195,000.00","190,000.00","194,940.50","83,003.60"),mean_2022 = c("160,000.00","44,677.00","100,550.74","127,187.27","170,196.60","137,136.68","196,979.00","118,187.00","129,451.94","58,255.00","141,766.67","162,674.00","105,569.00"),difference_20_21 = c(" -26%"," -128%"," 46%"," -3%"," -33%"," -26%"," -93%"," 50%"," -68%"," 74%"," -37%"," 24%"," -196%"),difference_21_22 = c(" 77%"," -32%"," 17%"," 33%"," 16%"," 48%"," 14%"," -52%"," 42%"," -235%"," -34%"," -20%"," 21%"))
knitr::opts_chunk$set(echo = TRUE)
mean_salary <- data.frame(job_title = c("AI Scientist","Business Data Analyst","Data Analyst","Data Engineer","Data Science Manager","Data Scientist","Director of Data Science","Lead Data Engineer","Machine Learning Engineer","Machine Learning Infrastructure Engineer","Machine Learning Scientist","Principal Data Scientist","Research Scientist"), mean_2020 = c("45,896.00","135,000.00","45,547.29","88,162.00","190,200.00","89,185.60","325,000.00","90,500.00","125,389.80","50,180.00","260,000.00","148,261.00","246,000.00"),mean_2021 = c("36,526.50","59,102.00","83,827.38","85,758.00","143,126.50","70,671.73","168,707.80","179,720.00","74,611.22","195,000.00","190,000.00","194,940.50","83,003.60"),mean_2022 = c("160,000.00","44,677.00","100,550.74","127,187.27","170,196.60","137,136.68","196,979.00","118,187.00","129,451.94","58,255.00","141,766.67","162,674.00","105,569.00"),difference_20_21 = c(" -26%"," -128%"," 46%"," -3%"," -33%"," -26%"," -93%"," 50%"," -68%"," 74%"," -37%"," 24%"," -196%"),difference_21_22 = c(" 77%"," -32%"," 17%"," 33%"," 16%"," 48%"," 14%"," -52%"," 42%"," -235%"," -34%"," -20%"," 21%"))
print(mean_salary)
library(tidyverse)
library(kableextra)
install.packages("zoo")
library(zoo)
install.packages("kableExtra")
ggplot() + geom_line(aes(y=job_title, x=))
getwd()
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
knitr::opts_knit$set(root.dir= normalizePath('..'))
getwd()
knitr::include_graphics("Mean_salary.png",error = FALSE)
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
knitr::opts_knit$set(root.dir= normalizePath('..'))
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
knitr::opts_knit$set(root.dir= normalizePath('..'))
getwd()
knitr::include_graphics("Mean_salary.png",error = FALSE)
update.packages(ask = FALSE, checkBuilt = TRUE)
tinytex::tlmgr_update()
knitr::include_graphics("Mean_salary.png",error = FALSE)
tinytex::reinstall_tinytex(repository = "illinois")
getwd()
knitr::include_graphics("Mean_salary.png",error = FALSE)
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
knitr::opts_knit$set(root.dir= normalizePath('..'))
knitr::include_graphics("Mean_salary.png",error = FALSE)
knitr::include_graphics("Mean_salary.png",error = FALSE)
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
knitr::opts_knit$set(root.dir= normalizePath('..'))
# Setup script for Project
# Please highlight line 4, 5, 6 & 7 and press "Run" button on top right or press "Source"
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
setwd("..")
library(ProjectTemplate)
load.project()
# Load the required libraries
library(ggplot2)
library(RColorBrewer)
library(readr)
library(MASS)
library(tidyverse)
library(readxl)
library(dplyr)
# Load the gaze data
PNL <- read_csv("data/PNL/gaze_data.csv", col_names = FALSE)
# Set column names
col_names <- c("Left Eye X", "Left Eye Y", "Right Eye X", "Right Eye Y")
colnames(PNL) <- col_names
# Remove NaN rows from the data
PNL <- PNL[complete.cases(PNL), ]
# Calculate the average gaze point between left and right eyes
PNL$Average_X <- (PNL$`Left Eye X` + PNL$`Right Eye X`) / 2
PNL$Average_Y <- ((PNL$`Left Eye Y` + PNL$`Right Eye Y`) / 2) * -1
# Ensure columns are numeric
PNL$Average_X <- as.numeric(PNL$Average_X)
PNL$Average_Y <- as.numeric(PNL$Average_Y)
# Extract the relevant columns for clustering
data_to_cluster_p <- PNL[, c("Average_X", "Average_Y")]
# Apply k-means clustering
set.seed(123)  # for reproducibility
k3_p <- kmeans(data_to_cluster_p, centers = 3)
# Add cluster assignments to original data
PNL$cluster <- k3_p$cluster
# Calculate the density and retrieve top 3 density levels
dens <- kde2d(PNL$Average_X, PNL$Average_Y, n = 100)
top_levels <- sort(unique(dens$z), decreasing = TRUE)[1:3]
# Create a KDE plot and overlay contours for top 3 density levels
heatmap_PNL <- ggplot(PNL, aes(x = Average_X, y = Average_Y)) +
stat_density_2d(aes(fill = ..level..), geom = "polygon", h = 0.08) +
stat_density_2d(geom = "contour", breaks = top_levels, color = "green", linewidth = 1) +
scale_fill_gradientn(colors = RColorBrewer::brewer.pal(9, "YlOrRd")) +
coord_cartesian(xlim = c(0, 1), ylim = c(-1, 0)) +
theme_minimal() +
theme(legend.position = "none") +
ggtitle("PNL - Average")
# Print the heatmap
print(heatmap_PNL)
# Load the gaze data
sankey <- read_csv("data/sankey/gaze_data.csv", col_names = FALSE)
# Set column names
colnames(sankey) <- col_names
# Remove NaN rows from the data
sankey <- sankey[complete.cases(sankey), ]
# Calculate the average gaze point between left and right eyes
sankey$Average_X <- (sankey$`Left Eye X` + sankey$`Right Eye X`) / 2
sankey$Average_Y <- ((sankey$`Left Eye Y` + sankey$`Right Eye Y`) / 2) * -1
# Ensure columns are numeric
sankey$Average_X <- as.numeric(sankey$Average_X)
sankey$Average_Y <- as.numeric(sankey$Average_Y)
# Extract the relevant columns for clustering
data_to_cluster_s <- sankey[, c("Average_X", "Average_Y")]
# Apply k-means clustering
set.seed(123)  # for reproducibility
k3_s <- kmeans(data_to_cluster_s, centers = 3)
# Add cluster assignments to original data
sankey$cluster <- k3_s$cluster
# Calculate the density and retrieve top 3 density levels
dens <- kde2d(sankey$Average_X, sankey$Average_Y, n = 100)
top_levels <- sort(unique(dens$z), decreasing = TRUE)[1:3]
# Create a KDE plot and overlay contours for top 3 density levels
heatmap_sankey <- ggplot(sankey, aes(x = Average_X, y = Average_Y)) +
stat_density_2d(aes(fill = ..level..), geom = "polygon", h = 0.09) +
stat_density_2d(geom = "contour", breaks = top_levels, color = "green", linewidth = 1) +
scale_fill_gradientn(colors = RColorBrewer::brewer.pal(9, "YlOrRd")) +
coord_cartesian(xlim = c(0, 1), ylim = c(-1, 0)) +
theme_minimal() +
theme(legend.position = "none") +
ggtitle("Sankey - Average")
# Print the heatmap
print(heatmap_sankey)
# Load the gaze data
waterfall <- read_csv("data/Waterfall/gaze_data.csv", col_names = FALSE)
# Set column names
colnames(waterfall) <- col_names
# Remove NaN rows from the data
waterfall <- waterfall[complete.cases(waterfall), ]
# Calculate the average gaze point between left and right eyes
waterfall$Average_X <- (waterfall$`Left Eye X` + waterfall$`Right Eye X`) / 2
waterfall$Average_Y <- ((waterfall$`Left Eye Y` + waterfall$`Right Eye Y`) / 2) * -1
# Ensure columns are numeric
waterfall$Average_X <- as.numeric(waterfall$Average_X)
waterfall$Average_Y <- as.numeric(waterfall$Average_Y)
# Extract the relevant columns for clustering
data_to_cluster_w <- waterfall[, c("Average_X", "Average_Y")]
# Apply k-means clustering
set.seed(123)  # for reproducibility
k3_w <- kmeans(data_to_cluster_w, centers = 3)
# Add cluster assignments to original data
waterfall$cluster <- k3_w$cluster
# Calculate the density and retrieve top 3 density levels
dens <- kde2d(waterfall$Average_X, waterfall$Average_Y, n = 100)
top_levels <- sort(unique(dens$z), decreasing = TRUE)[1:3]
# Create a KDE plot and overlay contours for top 3 density levels
heatmap_waterfall <- ggplot(waterfall, aes(x = Average_X, y = Average_Y)) +
stat_density_2d(aes(fill = ..level..), geom = "polygon", h = 0.08) +
stat_density_2d(geom = "contour", breaks = top_levels, color = "green", linewidth = 1) +
scale_fill_gradientn(colors = RColorBrewer::brewer.pal(9, "YlOrRd")) +
coord_cartesian(xlim = c(0, 1), ylim = c(-1, 0)) +
theme_minimal() +
theme(legend.position = "none") +
ggtitle("Waterfall - Average")
# Print the heatmap
print(heatmap_waterfall)
# Load the survey results data
Results <- read_excel("data/Results.xlsx")
# Select only 2 columns for quads
quads <- Results[, c("How often do you review financial data?", "How often do you review data visualisations?")]
quads[is.na(quads)] <- 5
# Normalising data from -2 to 2
quads$`How often do you review financial data?` <- (quads$`How often do you review financial data?` -3)*-1
quads$`How often do you review data visualisations?` <- (quads$`How often do you review data visualisations?`-3)*-1
# Summarize the number of points at each coordinate
quads_summary <- quads %>%
group_by(`How often do you review financial data?`, `How often do you review data visualisations?`) %>%
summarize(count = n()) %>%
ungroup()
# Plot
ggplot(quads_summary, aes(x=`How often do you review financial data?`, y=`How often do you review data visualisations?`, size=count)) +
geom_point(alpha = 0.7, colour = "blue") +  # added alpha for better visibility when points overlap
geom_hline(yintercept = 0, linetype="solid", color = "black") +
geom_vline(xintercept = 0, linetype="solid", color = "black") +
theme_minimal() +
labs(x="How often do you review financial data?", y="How often do you review data visualisations?", title="Scatter Plot of Quads") +
scale_size_continuous(range = c(1, 8), guide = "legend") + # adjust point sizes
xlim(-2, 2) +
ylim(-2, 2)
# Load the survey results data
Results <- read_excel("data/Results.xlsx")
# Select only 2 columns for quads
quads <- Results[, c("How often do you review financial data?", "How often do you review data visualisations?")]
quads[is.na(quads)] <- 5
# Normalising data from -2 to 2
quads$`How often do you review financial data?` <- (quads$`How often do you review financial data?` -3)*-1
quads$`How often do you review data visualisations?` <- (quads$`How often do you review data visualisations?`-3)*-1
# Summarize the number of points at each coordinate
quads_summary <- quads %>%
group_by(`How often do you review financial data?`, `How often do you review data visualisations?`) %>%
summarize(count = n()) %>%
ungroup()
# Plot
ggplot(quads_summary, aes(x=`How often do you review financial data?`, y=`How often do you review data visualisations?`, size=count)) +
geom_point(alpha = 0.7, colour = "blue") +  # added alpha for better visibility when points overlap
geom_hline(yintercept = 0, linetype="solid", color = "black") +
geom_vline(xintercept = 0, linetype="solid", color = "black") +
theme_minimal() +
labs(x="How often do you review financial data?", y="How often do you review data visualisations?", title="Representation of Expertise") +
scale_size_continuous(range = c(1, 8), guide = "legend") + # adjust point sizes
xlim(-2, 2) +
ylim(-2, 2)
# Load the survey results data
Results <- read_excel("data/Results.xlsx")
# Select only 2 columns for quads
quads <- Results[, c("How often do you review financial data?", "How often do you review data visualisations?")]
quads[is.na(quads)] <- 5
# Normalising data from -2 to 2
quads$`How often do you review financial data?` <- (quads$`How often do you review financial data?` -3)*-1
quads$`How often do you review data visualisations?` <- (quads$`How often do you review data visualisations?`-3)*-1
# Summarize the number of points at each coordinate
quads_summary <- quads %>%
group_by(`How often do you review financial data?`, `How often do you review data visualisations?`) %>%
summarize(count = n()) %>%
ungroup()
# Plot
ggplot(quads_summary, aes(x=`How often do you review financial data?`, y=`How often do you review data visualisations?`, size=count)) +
geom_point(alpha = 0.7, colour = "blue") +  # added alpha for better visibility when points overlap
geom_hline(yintercept = 0, linetype="solid", color = "black") +
geom_vline(xintercept = 0, linetype="solid", color = "black") +
theme_minimal() +
labs(x="How often do you review financial data?", y="How often do you review data visualisations?", title="Representation of Expertise") +
scale_size_continuous(range = c(1, 8), guide = "legend") + # adjust point sizes
xlim(-2, 2) +
ylim(-2, 2)
theme(axis.title.x = element_text(face = "bold"),  # Bold x label
axis.title.y = element_text(face = "bold"))  # Bold y label
# Load the survey results data
Results <- read_excel("data/Results.xlsx")
# Select only 2 columns for quads
quads <- Results[, c("How often do you review financial data?", "How often do you review data visualisations?")]
quads[is.na(quads)] <- 5
# Normalising data from -2 to 2
quads$`How often do you review financial data?` <- (quads$`How often do you review financial data?` -3)*-1
quads$`How often do you review data visualisations?` <- (quads$`How often do you review data visualisations?`-3)*-1
# Summarize the number of points at each coordinate
quads_summary <- quads %>%
group_by(`How often do you review financial data?`, `How often do you review data visualisations?`) %>%
summarize(count = n()) %>%
ungroup()
# Plot
ggplot(quads_summary, aes(x=`How often do you review financial data?`, y=`How often do you review data visualisations?`, size=count)) +
geom_point(alpha = 0.7, colour = "blue") +  # added alpha for better visibility when points overlap
geom_hline(yintercept = 0, linetype="solid", color = "black") +
geom_vline(xintercept = 0, linetype="solid", color = "black") +
theme_minimal() +
labs(x="How often do you review financial data?", y="How often do you review data visualisations?", title="Representation of Expertise") +
scale_size_continuous(range = c(1, 8), guide = "legend") + # adjust point sizes
xlim(-2, 2) +
ylim(-2, 2) +
theme(axis.title.x = element_text(face = "bold"),  # Bold x label
axis.title.y = element_text(face = "bold"))  # Bold y label
View(data_to_cluster_w)
View(quads)
View(Results)
PNL_e_1 <- read_csv("data/PNL/gaze_data_000_20230806_021656.csv", col_names = FALSE)
View(PNL_e_1)
PNL_e_1 <- read_csv("data/PNL/gaze_data_000_20230806_021656.csv", col_names = TRUE)
View(PNL_e_1)
PNL_e_2 <- read_csv("data/PNL/gaze_data_002_20230808_135758.csv", col_names = TRUE)
PNL_e_3 <- read_csv("data/PNL/gaze_data_008_20230813_174213.csv", col_names = TRUE)
PNL_e_4 <- read_csv("data/PNL/gaze_data_009_20230813_175544.csv", col_names = TRUE)
PNL_e_5 <- read_csv("data/PNL/gaze_data_011_20230814_112649.csv", col_names = TRUE)
PNL_e_6 <- read_csv("data/PNL/gaze_data_014_20230814_140714.csv", col_names = TRUE)
View(PNL_e_2)
View(PNL_e_3)
View(PNL_e_4)
PNL_e <- list(PNL_e_1,PNL_e_2,PNL_e_3,PNL_e_4,PNL_e_5,PNL_e_6)
View(PNL_e)
PNL_e <- bind_rows(PNL_e)
# Remove NaN rows from the data
PNL_e <- PNL_e[complete.cases(PNL_e), ]
View(PNL_e)
View(PNL_e)
# Calculate the average gaze point between left and right eyes
PNL_e$Average_X <- (PNL_e$`Left Eye X` + PNL_e$`Right Eye X`) / 2
PNL_e$Average_Y <- ((PNL_e$`Left Eye Y` + PNL_e$`Right Eye Y`) / 2) * -1
# Extract the relevant columns for clustering
data_to_cluster_pe <- PNL_e[, c("Average_X", "Average_Y")]
View(data_to_cluster_pe)
# Apply k-means clustering
set.seed(123)  # for reproducibility
k3_pe <- kmeans(data_to_cluster_pe, centers = 3)
View(k3_pe)
# Add cluster assignments to original data
PNL_e$cluster <- k3_pe$cluster
# Calculate the density and retrieve top 3 density levels
dens <- kde2d(PNL_e$Average_X, PNL$Average_Y, n = 100)
# Calculate the density and retrieve top 3 density levels
dens <- kde2d(PNL_e$Average_X, PNL_e$Average_Y, n = 100)
top_levels <- sort(unique(dens$z), decreasing = TRUE)[1:3]
# Create a KDE plot and overlay contours for top 3 density levels
heatmap_PNL_e <- ggplot(PNL_e, aes(x = Average_X, y = Average_Y)) +
stat_density_2d(aes(fill = ..level..), geom = "polygon", h = 0.08) +
stat_density_2d(geom = "contour", breaks = top_levels, color = "green", linewidth = 1) +
scale_fill_gradientn(colors = RColorBrewer::brewer.pal(9, "YlOrRd")) +
coord_cartesian(xlim = c(0, 1), ylim = c(-1, 0)) +
theme_minimal() +
theme(legend.position = "none") +
ggtitle("PNL - Expert")
# Print the heatmap
print(heatmap_PNL_e)
# Load the gaze data
PNL_n_1 <- read_csv("data/PNL/gaze_data_001_20230808_134037.csv", col_names = TRUE)
PNL_n_2 <- read_csv("data/PNL/gaze_data_003_20230808_162952.csv", col_names = TRUE)
PNL_n_3 <- read_csv("data/PNL/gaze_data_004_20230808_164451.csv", col_names = TRUE)
PNL_n_4 <- read_csv("data/PNL/gaze_data_005_20230808_165919.csv", col_names = TRUE)
PNL_n_5 <- read_csv("data/PNL/gaze_data_006_20230808_171002.csv", col_names = TRUE)
PNL_n_6 <- read_csv("data/PNL/gaze_data_007_20230813_171229.csv", col_names = TRUE)
PNL_n_7 <- read_csv("data/PNL/gaze_data_010_20230813_180426.csv", col_names = TRUE)
PNL_n_8 <- read_csv("data/PNL/gaze_data_012_20230814_113803.csv", col_names = TRUE)
PNL_n_9 <- read_csv("data/PNL/gaze_data_013_20230814_133051.csv", col_names = TRUE)
# Combine df
PNL_n <- list(PNL_n_1,PNL_n_2,PNL_n_3,PNL_n_4,PNL_n_5,PNL_n_6,PNL_n_7,PNL_n_8,PNL_n_9)
PNL_n <- bind_rows(PNL_n)
# Remove NaN rows from the data
PNL_n <- PNL_n[complete.cases(PNL_n), ]
# Calculate the average gaze point between left and right eyes
PNL_n$Average_X <- (PNL_n$`Left Eye X` + PNL_n$`Right Eye X`) / 2
PNL_n$Average_Y <- ((PNL_n$`Left Eye Y` + PNL_n$`Right Eye Y`) / 2) * -1
# Ensure columns are numeric
PNL_n$Average_X <- as.numeric(PNL_n$Average_X)
PNL_n$Average_Y <- as.numeric(PNL_n$Average_Y)
# Extract the relevant columns for clustering
data_to_cluster_pn <- PNL_n[, c("Average_X", "Average_Y")]
# Apply k-means clustering
set.seed(123)  # for reproducibility
k3_pn <- kmeans(data_to_cluster_pn, centers = 3)
# Add cluster assignments to original data
PNL_n$cluster <- k3_pn$cluster
# Calculate the density and retrieve top 3 density levels
dens <- kde2d(PNL_n$Average_X, PNL_n$Average_Y, n = 100)
top_levels <- sort(unique(dens$z), decreasing = TRUE)[1:3]
# Create a KDE plot and overlay contours for top 3 density levels
heatmap_PNL_n <- ggplot(PNL_n, aes(x = Average_X, y = Average_Y)) +
stat_density_2d(aes(fill = ..level..), geom = "polygon", h = 0.08) +
stat_density_2d(geom = "contour", breaks = top_levels, color = "green", linewidth = 1) +
scale_fill_gradientn(colors = RColorBrewer::brewer.pal(9, "YlOrRd")) +
coord_cartesian(xlim = c(0, 1), ylim = c(-1, 0)) +
theme_minimal() +
theme(legend.position = "none") +
ggtitle("PNL - Novice")
# Print the heatmap
print(heatmap_PNL_n)
########## Expert
# Load the gaze data
Sankey_e_1 <- read_csv("data/Sankey/gaze_data_000_20230806_021837.csv", col_names = TRUE)
Sankey_e_2 <- read_csv("data/Sankey/gaze_data_002_20230808_140129.csv", col_names = TRUE)
Sankey_e_3 <- read_csv("data/Sankey/gaze_data_008_20230813_174350.csv", col_names = TRUE)
Sankey_e_4 <- read_csv("data/Sankey/gaze_data_009_20230813_175934.csv", col_names = TRUE)
Sankey_e_5 <- read_csv("data/Sankey/gaze_data_011_20230814_112957.csv", col_names = TRUE)
Sankey_e_6 <- read_csv("data/Sankey/gaze_data_014_20230814_140830.csv", col_names = TRUE)
# Combine df
Sankey_e <- list(Sankey_e_1,Sankey_e_2,Sankey_e_3,Sankey_e_4,Sankey_e_5,Sankey_e_6)
Sankey_e <- bind_rows(Sankey_e)
# Remove NaN rows from the data
Sankey_e <- Sankey_e[complete.cases(Sankey_e), ]
# Calculate the average gaze point between left and right eyes
Sankey_e$Average_X <- (Sankey_e$`Left Eye X` + Sankey_e$`Right Eye X`) / 2
Sankey_e$Average_Y <- ((Sankey_e$`Left Eye Y` + Sankey_e$`Right Eye Y`) / 2) * -1
# Ensure columns are numeric
Sankey_e$Average_X <- as.numeric(Sankey_e$Average_X)
Sankey_e$Average_Y <- as.numeric(Sankey_e$Average_Y)
# Extract the relevant columns for clustering
data_to_cluster_se <- Sankey_e[, c("Average_X", "Average_Y")]
# Apply k-means clustering
set.seed(123)  # for reproducibility
k3_se <- kmeans(data_to_cluster_se, centers = 3)
# Add cluster assignments to original data
Sankey_e$cluster <- k3_se$cluster
# Calculate the density and retrieve top 3 density levels
dens <- kde2d(Sankey_e$Average_X, Sankey_e$Average_Y, n = 100)
top_levels <- sort(unique(dens$z), decreasing = TRUE)[1:3]
# Create a KDE plot and overlay contours for top 3 density levels
heatmap_Sankey_e <- ggplot(Sankey_e, aes(x = Average_X, y = Average_Y)) +
stat_density_2d(aes(fill = ..level..), geom = "polygon", h = 0.08) +
stat_density_2d(geom = "contour", breaks = top_levels, color = "green", linewidth = 1) +
scale_fill_gradientn(colors = RColorBrewer::brewer.pal(9, "YlOrRd")) +
coord_cartesian(xlim = c(0, 1), ylim = c(-1, 0)) +
theme_minimal() +
theme(legend.position = "none") +
ggtitle("Sankey - Expert")
# Print the heatmap
print(heatmap_Sankey_e)
# Load the gaze data
Sankey_n_1 <- read_csv("data/Sankey/gaze_data_001_20230808_134224.csv", col_names = TRUE)
Sankey_n_2 <- read_csv("data/Sankey/gaze_data_003_20230808_163204.csv", col_names = TRUE)
Sankey_n_3 <- read_csv("data/Sankey/gaze_data_004_20230808_164634.csv", col_names = TRUE)
Sankey_n_4 <- read_csv("data/Sankey/gaze_data_005_20230808_170051.csv", col_names = TRUE)
Sankey_n_5 <- read_csv("data/Sankey/gaze_data_006_20230808_171134.csv", col_names = TRUE)
Sankey_n_6 <- read_csv("data/Sankey/gaze_data_007_20230813_171520.csv", col_names = TRUE)
Sankey_n_7 <- read_csv("data/Sankey/gaze_data_010_20230813_180541.csv", col_names = TRUE)
Sankey_n_8 <- read_csv("data/Sankey/gaze_data_012_20230814_113925.csv", col_names = TRUE)
Sankey_n_9 <- read_csv("data/Sankey/gaze_data_014_20230814_140830.csv", col_names = TRUE)
# Combine df
Sankey_n <- list(Sankey_n_1,Sankey_n_2,Sankey_n_3,Sankey_n_4,Sankey_n_5,Sankey_n_6,Sankey_n_7,Sankey_n_8,Sankey_n_9)
Sankey_n <- bind_rows(Sankey_n)
# Remove NaN rows from the data
Sankey_n <- Sankey_n[complete.cases(Sankey_n), ]
# Calculate the average gaze point between left and right eyes
Sankey_n$Average_X <- (Sankey_n$`Left Eye X` + Sankey_n$`Right Eye X`) / 2
Sankey_n$Average_Y <- ((Sankey_n$`Left Eye Y` + Sankey_n$`Right Eye Y`) / 2) * -1
# Ensure columns are numeric
Sankey_n$Average_X <- as.numeric(Sankey_n$Average_X)
Sankey_n$Average_Y <- as.numeric(Sankey_n$Average_Y)
# Extract the relevant columns for clustering
data_to_cluster_sn <- Sankey_n[, c("Average_X", "Average_Y")]
# Apply k-means clustering
set.seed(123)  # for reproducibility
k3_sn <- kmeans(data_to_cluster_sn, centers = 3)
# Add cluster assignments to original data
Sankey_n$cluster <- k3_sn$cluster
# Calculate the density and retrieve top 3 density levels
dens <- kde2d(Sankey_n$Average_X, Sankey_n$Average_Y, n = 100)
top_levels <- sort(unique(dens$z), decreasing = TRUE)[1:3]
# Create a KDE plot and overlay contours for top 3 density levels
heatmap_Sankey_n <- ggplot(Sankey_n, aes(x = Average_X, y = Average_Y)) +
stat_density_2d(aes(fill = ..level..), geom = "polygon", h = 0.08) +
stat_density_2d(geom = "contour", breaks = top_levels, color = "green", linewidth = 1) +
scale_fill_gradientn(colors = RColorBrewer::brewer.pal(9, "YlOrRd")) +
coord_cartesian(xlim = c(0, 1), ylim = c(-1, 0)) +
theme_minimal() +
theme(legend.position = "none") +
ggtitle("Sankey - Novice")
# Print the heatmap
print(heatmap_Sankey_n)
########## Expert
# Load the gaze data
Waterfall_e_1 <- read_csv("data/Waterfall/gaze_data_000_20230806_025154.csv", col_names = TRUE)
Waterfall_e_2 <- read_csv("data/Waterfall/gaze_data_002_20230808_162820.csv", col_names = TRUE)
Waterfall_e_3 <- read_csv("data/Waterfall/gaze_data_008_20230813_174728.csv", col_names = TRUE)
Waterfall_e_4 <- read_csv("data/Waterfall/gaze_data_009_20230813_180059.csv", col_names = TRUE)
Waterfall_e_5 <- read_csv("data/Waterfall/gaze_data_011_20230814_113124.csv", col_names = TRUE)
Waterfall_e_6 <- read_csv("data/Waterfall/gaze_data_014_20230814_140954.csv", col_names = TRUE)
# Combine df
Waterfall_e <- list(Waterfall_e_1,Waterfall_e_2,Waterfall_e_3,Waterfall_e_4,Waterfall_e_5,Waterfall_e_6)
Waterfall_e <- bind_rows(Waterfall_e)
# Remove NaN rows from the data
Waterfall_e <- Waterfall_e[complete.cases(Waterfall_e), ]
# Calculate the average gaze point between left and right eyes
Waterfall_e$Average_X <- (Waterfall_e$`Left Eye X` + Waterfall_e$`Right Eye X`) / 2
Waterfall_e$Average_Y <- ((Waterfall_e$`Left Eye Y` + Waterfall_e$`Right Eye Y`) / 2) * -1
# Ensure columns are numeric
Waterfall_e$Average_X <- as.numeric(Waterfall_e$Average_X)
Waterfall_e$Average_Y <- as.numeric(Waterfall_e$Average_Y)
# Extract the relevant columns for clustering
data_to_cluster_we <- Waterfall_e[, c("Average_X", "Average_Y")]
# Apply k-means clustering
set.seed(123)  # for reproducibility
k3_we <- kmeans(data_to_cluster_we, centers = 3)
# Add cluster assignments to original data
Waterfall_e$cluster <- k3_we$cluster
# Calculate the density and retrieve top 3 density levels
dens <- kde2d(Waterfall_e$Average_X, Waterfall_e$Average_Y, n = 100)
top_levels <- sort(unique(dens$z), decreasing = TRUE)[1:3]
# Create a KDE plot and overlay contours for top 3 density levels
heatmap_Waterfall_e <- ggplot(Waterfall_e, aes(x = Average_X, y = Average_Y)) +
stat_density_2d(aes(fill = ..level..), geom = "polygon", h = 0.08) +
stat_density_2d(geom = "contour", breaks = top_levels, color = "green", linewidth = 1) +
scale_fill_gradientn(colors = RColorBrewer::brewer.pal(9, "YlOrRd")) +
coord_cartesian(xlim = c(0, 1), ylim = c(-1, 0)) +
theme_minimal() +
theme(legend.position = "none") +
ggtitle("Waterfall - Expert")
# Print the heatmap
print(heatmap_Waterfall_e)
########## Novice
# Load the gaze data
Waterfall_n_1 <- read_csv("data/Waterfall/gaze_data_001_20230808_134420.csv", col_names = TRUE)
Waterfall_n_2 <- read_csv("data/Waterfall/gaze_data_003_20230808_163400.csv", col_names = TRUE)
Waterfall_n_3 <- read_csv("data/Waterfall/gaze_data_004_20230808_165046.csv", col_names = TRUE)
Waterfall_n_4 <- read_csv("data/Waterfall/gaze_data_005_20230808_170227.csv", col_names = TRUE)
Waterfall_n_5 <- read_csv("data/Waterfall/gaze_data_006_20230808_171342.csv", col_names = TRUE)
Waterfall_n_6 <- read_csv("data/Waterfall/gaze_data_007_20230813_171739.csv", col_names = TRUE)
Waterfall_n_7 <- read_csv("data/Waterfall/gaze_data_010_20230813_180706.csv", col_names = TRUE)
Waterfall_n_8 <- read_csv("data/Waterfall/gaze_data_012_20230814_114046.csv", col_names = TRUE)
Waterfall_n_9 <- read_csv("data/Waterfall/gaze_data_014_20230814_140954.csv", col_names = TRUE)
# Combine df
Waterfall_n <- list(Waterfall_n_1,Waterfall_n_2,Waterfall_n_3,Waterfall_n_4,Waterfall_n_5,Waterfall_n_6,Waterfall_n_7,Waterfall_n_8,Waterfall_n_9)
Waterfall_n <- bind_rows(Waterfall_n)
# Remove NaN rows from the data
Waterfall_n <- Waterfall_n[complete.cases(Waterfall_n), ]
# Calculate the average gaze point between left and right eyes
Waterfall_n$Average_X <- (Waterfall_n$`Left Eye X` + Waterfall_n$`Right Eye X`) / 2
Waterfall_n$Average_Y <- ((Waterfall_n$`Left Eye Y` + Waterfall_n$`Right Eye Y`) / 2) * -1
# Ensure columns are numeric
Waterfall_n$Average_X <- as.numeric(Waterfall_n$Average_X)
Waterfall_n$Average_Y <- as.numeric(Waterfall_n$Average_Y)
# Extract the relevant columns for clustering
data_to_cluster_wn <- Waterfall_n[, c("Average_X", "Average_Y")]
# Apply k-means clustering
set.seed(123)  # for reproducibility
k3_wn <- kmeans(data_to_cluster_wn, centers = 3)
# Add cluster assignments to original data
Waterfall_n$cluster <- k3_wn$cluster
# Calculate the density and retrieve top 3 density levels
dens <- kde2d(Waterfall_n$Average_X, Waterfall_n$Average_Y, n = 100)
top_levels <- sort(unique(dens$z), decreasing = TRUE)[1:3]
# Create a KDE plot and overlay contours for top 3 density levels
heatmap_Waterfall_n <- ggplot(Waterfall_n, aes(x = Average_X, y = Average_Y)) +
stat_density_2d(aes(fill = ..level..), geom = "polygon", h = 0.08) +
stat_density_2d(geom = "contour", breaks = top_levels, color = "green", linewidth = 1) +
scale_fill_gradientn(colors = RColorBrewer::brewer.pal(9, "YlOrRd")) +
coord_cartesian(xlim = c(0, 1), ylim = c(-1, 0)) +
theme_minimal() +
theme(legend.position = "none") +
ggtitle("Waterfall - Novice")
# Print the heatmap
print(heatmap_Waterfall_n)
